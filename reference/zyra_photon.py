# -*- coding: utf-8 -*-
"""ZyRA_Photon_Colab_Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVClMJj4vOAcL9z21aQjW_cbtVpEfrmj

# ðŸŒŸ ZyRAâ€‘Photon: Resonant Photonic Intelligence â€” Colab Demo

**Author:** N01ne (Richard A. Aragon)  
**Tagline:** *I donâ€™t claim to have solved physics or AI. I claim it works in practice.*  
**Goal:** A runnable, minimal-yet-complete demo combining my working hypotheses about **resonance**, **spectral dynamics**, **photonic computation**, **semantic fields**, and **agentic consensus**. This is the tradition: launch with a Colab.

---

### What this notebook shows
1. **Spectral Decoherence:** eigenvalue-spacing variance and spectral entropy vs. decoherence.\
2. **Semantic Fields:** encode phrases â†’ project to fields â†’ decoherence â†’ decode â†’ fidelity curve.\
3. **Resonant Feedback:** echo and topology shift based on spectral entropy.\
4. **Selfâ€‘Organizing Light Logic:** two-beam Kerr-style simulation yielding an **AND** gate.\
5. **SupremeQueen Consensus:** a tiny multi-agent layer that improves detection stability.\
6. **Quick Benchmarks:** time-to-stable-gate (TTSG), simple error analysis.

> **Philosophy:** Push compute into **physics** (light, interference, nonlinearity). Use **ZRIA**-like control to shape resonance and memory. Let **agents** arbitrate symbols.

## 1) Setup
"""

#@title Install dependencies (Colab)
# If you're on Colab, this will pull the lightest viable stack for the demo.
# Re-run if the runtime restarts.
!pip -q install numpy scipy matplotlib scikit-learn sentence-transformers tqdm

#@title Imports & Globals
import os, math, time, random, json
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

from scipy.fft import fft2, ifft2
from scipy.ndimage import gaussian_filter
from scipy.optimize import minimize
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

# Matplotlib defaults
plt.rcParams["figure.figsize"] = (7, 4)
plt.rcParams["figure.dpi"] = 110

# Reproducibility
SEED = 42
rng = np.random.default_rng(SEED)
random.seed(SEED)
np.random.seed(SEED)

DEVICE = "cpu"  # This demo is CPU-friendly; photonic sim is FFT-heavy but modestly sized.

"""## 2) Utils"""

def plot_series(x, ys, labels, title, xlabel, ylabel):
    plt.figure()
    for y, lab in zip(ys, labels):
        plt.plot(x, y, label=lab)
    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.title(title); plt.grid(True)
    if any(labels): plt.legend()
    plt.show()

def show_intensity_grid(images, titles=None, nrows=1):
    n = len(images)
    ncols = int(np.ceil(n / nrows))
    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))
    axes = np.array(axes).reshape(nrows, ncols)
    for i, img in enumerate(images):
        r, c = divmod(i, ncols)
        ax = axes[r, c]
        im = ax.imshow(img, cmap="inferno", interpolation="nearest")
        ax.axis("off")
        if titles and i < len(titles): ax.set_title(titles[i])
    plt.tight_layout()
    plt.show()

"""## 3) Spectral Decoherence"""

def generate_ordered_matrix(N):
    return np.diag(np.linspace(-1, 1, N))

def compute_eigenvalues(M):
    eigvals = np.linalg.eigvalsh(M)
    return np.sort(eigvals)

def spacing_stats(eigvals):
    spacings = np.diff(eigvals)
    return spacings, spacings.mean(), spacings.var()

def spectral_entropy(eigvals):
    p = np.abs(eigvals)
    p = p / (p.sum() + 1e-12)
    return -np.sum(p * np.log(p + 1e-12))

def spectral_decoherence_demo(N=256, i_idx=80, j_idx=170, alphas=np.linspace(0,1,40)):
    H0 = generate_ordered_matrix(N)
    phi1 = np.zeros(N); phi1[i_idx] = 1
    phi2 = np.zeros(N); phi2[j_idx] = 1
    psi_super = (phi1 + phi2) / np.sqrt(2)

    vars_, ents_ = [], []
    for a in alphas:
        rho_pure = np.outer(psi_super, psi_super.conj())
        rho_mixed = np.zeros_like(rho_pure)
        rho_mixed[i_idx,i_idx] = 0.5; rho_mixed[j_idx,j_idx] = 0.5

        H_proxy = (1-a)*np.outer(psi_super, H0 @ psi_super) + a*H0
        vals = compute_eigenvalues(H_proxy)
        _, _, v = spacing_stats(vals)
        vars_.append(v)
        ents_.append(spectral_entropy(vals))
    return np.array(vars_), np.array(ents_), alphas

vars_, ents_, alphas = spectral_decoherence_demo()
plot_series(alphas, [vars_, ents_], ["spacing var", "spectral entropy"],
            "Decoherence sweep (Î±) vs spectral stats", "Î± (decoherence)", "value")

"""## 4) Semantic Fields under Decoherence"""

# Attempt to use sentence-transformers; fallback to random if unavailable
USE_ST = True
try:
    from sentence_transformers import SentenceTransformer
    _st_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
except Exception as e:
    print("Sentence-Transformers unavailable or failed to load; using random embeddings.", e)
    USE_ST = False
    _st_model = None

def embed_text(text):
    if USE_ST and _st_model is not None:
        v = _st_model.encode([text])[0]
        v = v / (np.linalg.norm(v) + 1e-12)
        return v
    # fallback: seeded random embedding for reproducibility
    rng_local = np.random.default_rng(abs(hash(text)) % (2**32))
    v = rng_local.normal(size=384)
    return v / (np.linalg.norm(v) + 1e-12)

def project_to_field(vec, shape=(48,48), seed=123):
    H, W = shape
    D = len(vec)
    rng_local = np.random.default_rng(seed)
    P = rng_local.normal(size=(H*W, D))
    P = P / (np.linalg.norm(P, axis=0, keepdims=True) + 1e-12)
    field = (P @ vec).reshape(H, W)
    return field, P

def decode_from_field(field, P):
    vhat, *_ = np.linalg.lstsq(P, field.reshape(-1), rcond=None)
    vhat = vhat / (np.linalg.norm(vhat) + 1e-12)
    return vhat

def semantic_decoherence_demo(phrase1="quantum", phrase2="mixture", shape=(48,48), alphas=np.linspace(0,1,20)):
    v1 = embed_text(phrase1)
    v2 = embed_text(phrase2)
    F1, P1 = project_to_field(v1, shape=shape, seed=11)
    F2, P2 = project_to_field(v2, shape=shape, seed=29)

    sims1, sims2 = [], []
    for a in alphas:
        superF = (F1 + F2)/np.sqrt(2)
        F1d = (1-a)*superF + a*F1
        F2d = (1-a)*superF + a*F2
        v1hat = decode_from_field(F1d, P1)
        v2hat = decode_from_field(F2d, P2)
        sims1.append(cosine_similarity([v1], [v1hat])[0][0])
        sims2.append(cosine_similarity([v2], [v2hat])[0][0])
    return np.array(sims1), np.array(sims2), alphas

s1, s2, al = semantic_decoherence_demo()
plot_series(al, [s1, s2], ["fidelity(phrase1)", "fidelity(phrase2)"],
            "Semantic decoding fidelity vs decoherence", "Î± (decoherence)", "cosine similarity")

"""## 5) Resonant Echo + Geometry Shift"""

def add_resonant_echo(H_proxy, field, gain=0.05):
    E = field.reshape(-1)
    return H_proxy + gain*np.outer(E, E)

def apply_geometry_shift(H_proxy, entropy_value, threshold=2.0, scale=0.03):
    if entropy_value > threshold:
        return H_proxy + rng.normal(scale=scale*np.sqrt(entropy_value), size=H_proxy.shape)
    return H_proxy

def echo_geom_demo(shape=(48,48), alphas=np.linspace(0,1,20)):
    # build a proxy from semantic fields
    v1 = embed_text("resonance"); v2 = embed_text("geometry")
    F1, P1 = project_to_field(v1, shape=shape, seed=7)
    F2, P2 = project_to_field(v2, shape=shape, seed=13)
    superF = (F1 + F2)/np.sqrt(2)
    H0 = np.diag(np.linspace(-1,1,shape[0]))

    ents, ents_echo, ents_both = [], [], []
    for a in alphas:
        H_proxy = (np.outer(F1.reshape(-1), F1.reshape(-1)) + np.outer(F2.reshape(-1), F2.reshape(-1))) / 2.0
        H0_emb = np.zeros_like(H_proxy)
        n = H0.shape[0]
        H0_emb[:n,:n] = H0
        H = (1-a)*H_proxy + a*H0_emb
        vals = compute_eigenvalues(H); S = spectral_entropy(vals); ents.append(S)

        H_e = add_resonant_echo(H, superF, gain=0.05)
        vals_e = compute_eigenvalues(H_e); S_e = spectral_entropy(vals_e); ents_echo.append(S_e)

        H_b = apply_geometry_shift(H_e, S_e, threshold=2.0, scale=0.03)
        vals_b = compute_eigenvalues(H_b); S_b = spectral_entropy(vals_b); ents_both.append(S_b)

    return np.array(ents), np.array(ents_echo), np.array(ents_both), alphas

e0, e1, e2, al = echo_geom_demo()
plot_series(al, [e0, e1, e2], ["baseline", "echo", "echo+geom"],
            "Spectral entropy under echo/geometry feedback", "Î± (decoherence)", "spectral entropy")

"""## 6) Selfâ€‘Organizing Light Logic (AND Gate)"""

# Parameters kept modest for Colab demo
BASE_SIZE = 96
TIMESTEPS = 30
WAVELENGTH = 0.12
KERR_STRENGTH = 12.0
SATURATION = 2.0
NOISE_LEVEL = 0.003

def initialize_light_field(size, amp1, amp2):
    x = np.linspace(-1, 1, size)
    X, Y = np.meshgrid(x, x)
    g1 = np.exp(-((X - 0.4)**2 + Y**2) / 0.05)
    g2 = np.exp(-((X + 0.4)**2 + Y**2) / 0.05)
    phase = np.exp(1j * 2*np.pi * X / WAVELENGTH)
    return (amp1*g1 + amp2*g2) * phase

def propagate_light(field, kerr_strength, target_energy):
    k = fft2(field)
    kx = np.fft.fftfreq(field.shape[0])[:, None]
    ky = np.fft.fftfreq(field.shape[1])[None, :]
    k2 = kx**2 + ky**2
    phase = np.exp(-1j * 2*np.pi * WAVELENGTH * k2)
    k *= phase
    lin = ifft2(k)

    I = np.abs(lin)**2
    nl = I / (1 + I / SATURATION)
    out = lin * np.exp(1j * kerr_strength * nl)

    noise = NOISE_LEVEL * (rng.standard_normal(out.shape) + 1j*rng.standard_normal(out.shape))
    out = out + noise

    E = np.sum(np.abs(out)**2)
    if E > 0:
        out *= np.sqrt(target_energy/E)
    return out

def intensity(field):
    return np.abs(field)**2

def detect_and(I):
    h, w = I.shape
    thr = max(0.1, 0.2*np.max(I))
    B = (I > thr).astype(np.int32)
    L = B[h//4:3*h//4, w//8:3*w//8].mean() > 0.15
    R = B[h//4:3*h//4, 5*w//8:7*w//8].mean() > 0.15
    return 1 if (L and R) else 0

def run_logic_case(amp1, amp2):
    F = initialize_light_field(BASE_SIZE, amp1, amp2)
    target_E = np.sum(intensity(F))
    outs = []
    frames = []
    stable_t = None
    last = None

    for t in range(TIMESTEPS):
        F = propagate_light(F, KERR_STRENGTH, target_E)
        I = intensity(F)
        frames.append(I)
        out = detect_and(I)
        outs.append(out)
        if last is None or out != last:
            stable_t = t  # reset stability timer
            last = out

    return outs[-1], stable_t, frames

truth = [((1,1),1), ((1,0),0), ((0,1),0), ((0,0),0)]
results = []
snapshots = []

for (a1,a2), target in truth:
    y, t_stable, frames = run_logic_case(a1, a2)
    results.append((a1,a2,target,y,t_stable))
    snapshots.append(frames[-1])

print("Case\tTarget\tPred\tTTSG (lower is better)")
for a1,a2,target,y,t_stable in results:
    print(f"{a1}&{a2}\t{target}\t{y}\t{t_stable}")

titles = [f"{a1}&{a2}â†’{y}" for (a1,a2,target,y,t_stable) in results]
show_intensity_grid(snapshots, titles=titles, nrows=1)

"""## 7) SupremeQueen Miniâ€‘Consensus"""

class Worker:
    def __init__(self, strategy):
        self.strategy = strategy
        self.acc = 0.5

    def vote(self, region, thr):
        mx = region.max()
        mn = region.mean()
        if self.strategy == "peak":
            return mx > thr
        if self.strategy == "mean":
            return mn > thr/3
        if self.strategy == "hybrid":
            return (mx > thr) * 0.7 + (mn > thr/3) * 0.3 > 0.6
        return mx > thr

def consensus_and(I, workers_left, workers_right, thr):
    h, w = I.shape
    Lreg = I[h//4:3*h//4, w//8:3*w//8]
    Rreg = I[h//4:3*h//4, 5*w//8:7*w//8]
    L = np.mean([float(w.vote(Lreg, thr)) for w in workers_left]) >= 0.5
    R = np.mean([float(w.vote(Rreg, thr)) for w in workers_right]) >= 0.5
    return 1 if (L and R) else 0

# Compare single detector vs. consensus
workers_left  = [Worker(s) for s in ["peak", "mean", "hybrid", "peak"]]
workers_right = [Worker(s) for s in ["peak", "mean", "hybrid", "peak"]]

def eval_consensus(amp1, amp2):
    F = initialize_light_field(BASE_SIZE, amp1, amp2)
    target_E = np.sum(intensity(F))
    outs_single, outs_cons = [], []
    thr = None

    for t in range(TIMESTEPS):
        F = propagate_light(F, KERR_STRENGTH, target_E)
        I = intensity(F)
        thr = max(0.1, 0.2*np.max(I))
        outs_single.append(detect_and(I))
        outs_cons.append(consensus_and(I, workers_left, workers_right, thr))

    return outs_single, outs_cons

# âœ… Fixed quotes in plotting
single, cons = eval_consensus(1, 1)
plt.figure()
plt.plot(single, label="single")
plt.plot(cons, label="consensus")
plt.title("AND output over time: single vs. consensus")
plt.xlabel("timestep")
plt.ylabel("output")
plt.grid(True)
plt.legend()
plt.show()

"""## 8) Quick Benchmarks & Report"""

# Build final report dict
report = {}

# Spectral slopes
report['spectral'] = {
    'spacing_var_start': float(vars_[0]),
    'spacing_var_end': float(vars_[-1]),
    'entropy_start': float(ents_[0]),
    'entropy_end': float(ents_[-1]),
}

# Semantic fidelity AUC (simple trapezoid)
auc1 = float(np.trapz(s1, al))
auc2 = float(np.trapz(s2, al))
report['semantic'] = {'fidelity_auc_phrase1': auc1, 'fidelity_auc_phrase2': auc2}

# Logic results
acc = np.mean([int(t == y) for (_, _, t, y, _) in results])
ttsg = {f"{a1}&{a2}": int(ts) for (a1, a2, _, _, ts) in results}
report['logic'] = {'accuracy_truth_table': float(acc), 'TTSG': ttsg}

print(json.dumps(report, indent=2))

