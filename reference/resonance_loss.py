# -*- coding: utf-8 -*-
"""Understanding Resonance-Based Loss in ZyRA Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZLOvEaAVIsnhXbhYWYLA8jgXuonGSF1_

### üìò **Tutorial: Understanding Resonance-Based Loss in ZyRA Models**

#### 1. **Introduction**
- What is ZyRA?
- Why traditional loss functions (cross-entropy, MSE, etc.) aren't enough.
- Motivation for resonance-based loss: capturing alignment across hierarchical and spectral space.

#### 2. **What is Resonance in the Context of AI?**
- Not just a metaphor‚Äîresonance represents harmonic alignment between signal pathways in layers.
- Inspired by physics: systems that ‚Äúresonate‚Äù transmit energy/information more efficiently.
- In ZyRA, resonance is modeled between latent clusters, embeddings, and attention flows.

#### 3. **Core Components of Resonance-Based Loss**
- **Spectral Alignment Loss**: Encourages harmonic coherence between high-dimensional embeddings (e.g., via Fourier or Laplacian spectra).
- **Feedback Loop Coherence**: Penalizes divergence between forward pass representations and echo-like feedback layers.
- **Resonant Entropy Loss**: Rewards structure-preserving variability rather than pure minimization of error.
- **Cluster Field Similarity (RCF Tie-in)**: A loss component for ensuring stability in semantically entangled vector fields.

#### 4. **Mathematical Formulation (Simplified)**
```python
resonance_loss = (
    alpha * spectral_alignment_loss(output_embedding, reference_spectrum) +
    beta * feedback_coherence_loss(forward_repr, feedback_repr) +
    gamma * resonant_entropy_loss(hidden_state_dynamics) +
    delta * cluster_similarity_loss(field_vectors, harmonic_centroids)
)
```

- `alpha, beta, gamma, delta` are tunable weights.
- Each subcomponent loss could be implemented with variants of cosine similarity, KL divergence, or energy-based functions.

#### 5. **Visualizing Resonance**
- Example plots: Alignment between eigenvectors before/after training.
- Frequency heatmaps of feedback oscillations across layers.
- Contrastive plots of conventional loss vs resonance-based loss on paraphrase tasks.

#### 6. **Benefits Over Conventional Loss Functions**
- Better semantic coherence.
- Robust generalization to hard paraphrase or low-signal datasets.
- Natural gradient flow in curved (e.g., hyperbolic or mixed-geometry) space.

#### 7. **Code Snippet: Simple Resonance Loss in PyTorch**
```python
import torch
import torch.nn.functional as F

def spectral_alignment_loss(embedding, target_spectrum):
    embedding_fft = torch.fft.fft(embedding, dim=-1)
    return F.mse_loss(torch.abs(embedding_fft), target_spectrum)

def feedback_coherence_loss(forward, feedback):
    return 1 - F.cosine_similarity(forward, feedback).mean()

def resonant_entropy_loss(state_dynamics):
    return -torch.mean(torch.var(state_dynamics, dim=1))

def resonance_loss_fn(output_emb, ref_spec, forward, feedback, dynamics, field_vecs, centroids,
                      alpha=0.3, beta=0.3, gamma=0.2, delta=0.2):
    loss = (
        alpha * spectral_alignment_loss(output_emb, ref_spec) +
        beta * feedback_coherence_loss(forward, feedback) +
        gamma * resonant_entropy_loss(dynamics) +
        delta * F.mse_loss(field_vecs, centroids)
    )
    return loss
```

#### 8. **Tips for Tuning and Debugging**
- How to visualize resonance curves during training.
- When to increase entropy weighting vs spectral alignment.
- Common failure modes and how to address them.

#### 9. **Use Cases and Experimental Results**
- Show ZyRA‚Äôs performance on standard vs hard paraphrase datasets.
- Discuss how resonance loss improved harmonic clustering in sparse attention heads.

#### 10. **Conclusion**
- Resonance loss isn‚Äôt just regularization‚Äîit‚Äôs a different *lens* on learning.
- It bridges symbolic alignment and geometry-aware signal processing.
"""

# --- RESONANCE-BASED LOSS EXPLORATION NOTEBOOK ---

# üì¶ Install and import necessary libraries
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# üîß Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)

# --- DATA SIMULATION SECTION ---

# 1. Simulate model embeddings and reference spectrum
embedding = torch.randn(100, 64)
ref_spectrum = torch.abs(torch.fft.fft(torch.randn(64)))

# 2. Simulate forward and feedback layer representations
forward_repr = torch.randn(100, 64)
feedback_repr = forward_repr + torch.randn(100, 64) * 0.1

# 3. Simulate state dynamics (as evolving hidden states over time)
state_dynamics = torch.randn(100, 32) + torch.sin(torch.linspace(0, np.pi, 32))

# 4. Simulate cluster field vectors and harmonic centroids
field_vectors = torch.randn(100, 64)
harmonic_centroids = field_vectors.mean(dim=0, keepdim=True)

# --- LOSS FUNCTION DEFINITIONS ---

def spectral_alignment_loss(embedding, target_spectrum):
    embedding_fft = torch.fft.fft(embedding, dim=-1)
    return F.mse_loss(torch.abs(embedding_fft), target_spectrum)

def feedback_coherence_loss(forward, feedback):
    return 1 - F.cosine_similarity(forward, feedback).mean()

def resonant_entropy_loss(state_dynamics):
    return -torch.mean(torch.var(state_dynamics, dim=1))

def cluster_similarity_loss(field_vectors, centroids):
    return F.mse_loss(field_vectors, centroids)

def resonance_loss_fn(output_emb, ref_spec, forward, feedback, dynamics, field_vecs, centroids,
                      alpha=0.3, beta=0.3, gamma=0.2, delta=0.2):
    loss = (
        alpha * spectral_alignment_loss(output_emb, ref_spec) +
        beta * feedback_coherence_loss(forward, feedback) +
        gamma * resonant_entropy_loss(dynamics) +
        delta * cluster_similarity_loss(field_vecs, centroids)
    )
    return loss

# --- COMPUTE LOSS COMPONENTS ---
spectral_loss = spectral_alignment_loss(embedding, ref_spectrum)
feedback_loss = feedback_coherence_loss(forward_repr, feedback_repr)
entropy_loss = resonant_entropy_loss(state_dynamics)
cluster_loss = cluster_similarity_loss(field_vectors, harmonic_centroids)
resonance_loss = resonance_loss_fn(embedding, ref_spectrum, forward_repr, feedback_repr,
                                   state_dynamics, field_vectors, harmonic_centroids)

# --- VISUALIZATIONS SECTION ---

# üìä Spectrum Alignment Plot
embedding_fft = torch.abs(torch.fft.fft(embedding.mean(dim=0)))
plt.figure(figsize=(10, 5))
plt.plot(embedding_fft.numpy(), label='Embedding Spectrum')
plt.plot(ref_spectrum.numpy(), label='Reference Spectrum', linestyle='--')
plt.title("Spectral Alignment")
plt.xlabel("Frequency Component")
plt.ylabel("Magnitude")
plt.legend()
plt.grid(True)
plt.show()

# üìä Cosine Similarity Histogram (Forward vs Feedback)
cos_sim = F.cosine_similarity(forward_repr, feedback_repr, dim=1)
plt.figure(figsize=(10, 5))
plt.hist(cos_sim.detach().numpy(), bins=30, alpha=0.7, color='skyblue')
plt.title("Feedback vs Forward Cosine Similarity")
plt.xlabel("Cosine Similarity")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# üìä Hidden State Dynamics Variance Plot
state_var = torch.var(state_dynamics, dim=1).detach().numpy()
plt.figure(figsize=(10, 5))
plt.plot(state_var, color='orange')
plt.title("Resonant Entropy: Variance of Hidden State Dynamics")
plt.xlabel("Sample Index")
plt.ylabel("Variance")
plt.grid(True)
plt.show()

# üìä Resonance Loss Components Summary Table
loss_values = {
    "Spectral Alignment Loss": spectral_loss.item(),
    "Feedback Coherence Loss": feedback_loss.item(),
    "Resonant Entropy Loss": entropy_loss.item(),
    "Cluster Similarity Loss": cluster_loss.item(),
    "Total Resonance Loss": resonance_loss.item()
}

loss_df = pd.DataFrame.from_dict(loss_values, orient='index', columns=['Loss Value'])
display(loss_df)

# üìò Summary Output
print("\n‚úÖ Resonance Loss Computation Complete.")
print(f"üî∏ Total Resonance Loss: {resonance_loss.item():.4f}")